---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: snapshot-cronjob-controller
  annotations:
    policies.kyverno.io/title: Snapshot CronJob controller
    policies.kyverno.io/subject: PersistentVolumeClaim
    policies.kyverno.io/description: >-
      This policy creates a snapshot CronJob for labeled PersistentVolumeClaims
spec:
  mutateExistingOnPolicyUpdate: true
  generateExistingOnPolicyUpdate: true
  rules:
  - name: create-snapshot-cronjob
    match:
      any:
      - resources:
          kinds:
          - PersistentVolumeClaim
          selector:
            matchLabels:
              snapshot.home.arpa/enabled: "true"
              app.kubernetes.io/name: "*"
              app.kubernetes.io/instance: "*"

    context:
    - name: claimName
      variable:
        jmesPath: "request.object.metadata.name"
    - name: namespace
      variable:
        jmesPath: "request.object.metadata.namespace || 'default'"
    - name: nodeAffinity
      variable:
        value:
          enabled: "{{ (request.object.metadata.labels.\"snapshot.home.arpa/nodeAffinity\" || 'true') == 'true' }}"
          labels:
          - key: "app.kubernetes.io/name"
            operator: "In"
            values:
            - "{{ request.object.metadata.labels.\"app.kubernetes.io/name\" }}"
          - key: "app.kubernetes.io/instance"
            operator: "In"
            values:
            - "{{ request.object.metadata.labels.\"app.kubernetes.io/instance\" }}"

    generate:
      synchronize: true
      apiVersion: batch/v1
      kind: CronJob
      name: "{{ claimName }}-snapshot"
      namespace: "{{ request.object.metadata.namespace }}"
      data:
        metadata:
          labels:
            app.kubernetes.io/name: "{{ request.object.metadata.labels.\"app.kubernetes.io/name\" }}"
            app.kubernetes.io/instance: "{{ request.object.metadata.labels.\"app.kubernetes.io/instance\" }}"
          ownerReferences:
          - apiVersion: "{{ request.object.apiVersion }}"
            kind: "{{ request.object.kind }}"
            name: "{{ request.object.metadata.name }}"
            uid: "{{ request.object.metadata.uid }}"
        spec:
          schedule: "@daily"
          suspend: false
          concurrencyPolicy: Forbid
          successfulJobsHistoryLimit: 1
          failedJobsHistoryLimit: 2
          jobTemplate:
            spec:
              ttlSecondsAfterFinished: 43200
              template:
                spec:
                  automountServiceAccountToken: false
                  restartPolicy: OnFailure
                  initContainers:
                  # Stagger jobs to run randomly within X seconds to avoid bringing down all apps at once
                  - name: wait
                    image: ghcr.io/onedr0p/alpine:3.16.2@sha256:e25d6744afe5ae0c03e0b2af4977c2d7df3f56b713c3b5cdffb8c372edf626be
                    command: ["/scripts/sleep.sh"]
                    args: ["1", "900"]

                  containers:
                  - name: backup
                    image: ghcr.io/onedr0p/kopia:0.12.1@sha256:e333295b519ce586e7c050c970b2255d87bdb2979298ff87ebdb1113e381ba3b
                    env:
                    - name: KOPIA_CACHE_DIRECTORY
                      value: /data/backups/cache/{{ namespace }}/{{ claimName }}
                    - name: KOPIA_LOG_DIR
                      value: /data/backups/logs/{{ namespace }}/{{ claimName }}
                    - name: KOPIA_PASSWORD
                      value: "none"
                    command:
                    - /bin/bash
                    - -c
                    - |-
                      printf "\e[1;32m%-6s\e[m\n" "[01/10] Create repo ..."              && [[ ! -f /data/backups/kopia.repository.f ]] && kopia repository create filesystem --path=/data/backups
                      printf "\e[1;32m%-6s\e[m\n" "[02/10] Connect to repo ..."          && kopia repo connect filesystem --path=/data/backups --override-hostname=cluster --override-username=root
                      printf "\e[1;32m%-6s\e[m\n" "[03/10] Set policies ..."             && kopia policy set /data/{{ namespace }}/{{ claimName }} --compression=zstd --keep-latest 14 --keep-hourly 0 --keep-daily 7 --keep-weekly 2 --keep-monthly 0 --keep-annual 0
                      printf "\e[1;32m%-6s\e[m\n" "[04/10] Freeze {{ claimName }} ..."   && fsfreeze -f /data/{{ namespace }}/{{ claimName }}
                      printf "\e[1;32m%-6s\e[m\n" "[05/10] Snapshot {{ claimName }} ..." && kopia snap create /data/{{ namespace }}/{{ claimName }}
                      printf "\e[1;32m%-6s\e[m\n" "[06/10] Unfreeze {{ claimName }} ..." && fsfreeze -u /data/{{ namespace }}/{{ claimName }}
                      printf "\e[1;32m%-6s\e[m\n" "[07/10] List snapshots ..."           && kopia snap list /data/{{ namespace }}/{{ claimName }}
                      printf "\e[1;32m%-6s\e[m\n" "[08/10] Show stats ..."               && kopia content stats
                      printf "\e[1;32m%-6s\e[m\n" "[09/10] Show maintenance info ..."    && kopia maintenance info
                      printf "\e[1;32m%-6s\e[m\n" "[10/10] Disconnect from repo ..."     && kopia repo disconnect
                    securityContext:
                      privileged: true
                    volumeMounts:
                    - name: appdata
                      mountPath: "/data/{{ namespace }}/{{ claimName }}"
                    - name: backup
                      mountPath: /data/backups
                  volumes:
                  - name: appdata
                    persistentVolumeClaim:
                      claimName: "{{ claimName }}"
                  - name: backup
                    nfs:
                      server: "${NAS_ADDRESS}"
                      path: "/volume1/Kopia"

                  affinity:
                    podAffinity:
                      requiredDuringSchedulingIgnoredDuringExecution:
                      - topologyKey: kubernetes.io/hostname
                        labelSelector:
                          matchExpressions: "{{ nodeAffinity.enabled && nodeAffinity.labels || [] }}"
